{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_gd2Beuoa45u"
      },
      "outputs": [],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "T05_3JX9XF_o"
      },
      "outputs": [],
      "source": [
        "!pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFlYKeiu11wR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from tqdm import tqdm\n",
        "from warnings import filterwarnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word, TextBlob\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        "    BertTokenizer,\n",
        "    BertModel,\n",
        "    BertForSequenceClassification\n",
        ")\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "from peft import LoraConfig, PeftConfig\n",
        "\n",
        "from trl import SFTTrainer, setup_chat_format\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "pd.set_option('display.width', 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu4wk19wPl-8"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rOzesxJ11y3"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/CombinedData.csv\",index_col=0, sep=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg8YzArJ111c"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-1ZCri2113u"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7OocSbH115_"
      },
      "outputs": [],
      "source": [
        "data[\"status\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDfTQJ-B4PIy"
      },
      "outputs": [],
      "source": [
        "status_counts = data[\"status\"].value_counts()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "status_counts.plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    startangle=90,\n",
        "    colors=plt.cm.Set3.colors,\n",
        "    title=\"Status Distribution\"\n",
        ")\n",
        "plt.ylabel(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-vQN-zz4SpZ"
      },
      "outputs": [],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDAUuOz-4Wht"
      },
      "outputs": [],
      "source": [
        "data = data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCyly-T94Y3m"
      },
      "outputs": [],
      "source": [
        "data.statement.duplicated(keep=\"first\").value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Zu7ILTa4gvT"
      },
      "outputs": [],
      "source": [
        "data = data.drop_duplicates(subset=['statement'], keep=\"first\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-11JibVGymZs"
      },
      "outputs": [],
      "source": [
        "target_count = 5000\n",
        "\n",
        "balanced_data = data.groupby(\"status\").apply(\n",
        "    lambda x: x.sample(min(len(x), target_count), random_state=42)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(\"Original Class Distribution:\")\n",
        "print(data[\"status\"].value_counts())\n",
        "\n",
        "print(\"\\nBalanced Class Distribution:\")\n",
        "print(balanced_data[\"status\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnZkBRZuymcA"
      },
      "outputs": [],
      "source": [
        "balanced_data['statement_length'] = balanced_data['statement'].apply(len)\n",
        "plt.figure(figsize=(10, 3))\n",
        "sns.histplot(balanced_data['statement_length'], bins=50, kde=True)\n",
        "plt.title('Distribution of Statement Lengths')\n",
        "plt.xlabel('Length of Statement_length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGz6EiOe08t1"
      },
      "outputs": [],
      "source": [
        "balanced_data['words'] = [len(x.split()) for x in balanced_data['statement'].tolist()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkJBnMz30is8"
      },
      "outputs": [],
      "source": [
        "balanced_data[['words','statement']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDDcd0qX1PDV"
      },
      "outputs": [],
      "source": [
        "balanced_data['words'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wd0C2-k1bZT"
      },
      "outputs": [],
      "source": [
        "min_statement_size=16\n",
        "balanced_data[balanced_data[\"words\"] < min_statement_size].count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrzubNaX1wlL"
      },
      "outputs": [],
      "source": [
        "df = balanced_data[balanced_data['words'] > min_statement_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOIYzDza18GW"
      },
      "outputs": [],
      "source": [
        "df['status'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E-Pj5s54gx-"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()  #Lowercase text\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove links\n",
        "    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n",
        "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\n', '', text)  # Remove newlines\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)  # Remove words containing numbers\n",
        "    return text\n",
        "\n",
        "df['statement'] = df['statement'].apply(lambda x: preprocess_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwQ6HCQT4g0Z"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQIBzLH_4g22"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    stop_words = stopwords.words('english')\n",
        "    text = text.apply(lambda x: \" \".join(x for x in str(x).split() if x not in stop_words))\n",
        "    return text\n",
        "\n",
        "df['statement'] = remove_stopwords(df['statement'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o8n1V234luu"
      },
      "outputs": [],
      "source": [
        "rare_values = pd.Series(' '.join(df[\"statement\"]).split()).value_counts()[-30:]\n",
        "print(len(rare_values))\n",
        "df[\"statement\"] = df[\"statement\"].apply(lambda x: \" \".join(x for x in x.split() if x not in rare_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMSwShhG4lxG"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTRZ_kLC4lzR"
      },
      "outputs": [],
      "source": [
        "#!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1P-zKzji4wXD"
      },
      "outputs": [],
      "source": [
        "df['statement'] = df['statement'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9PIuhnw4wUr"
      },
      "outputs": [],
      "source": [
        "def generate_wordcloud(dataframe, text_column, group_column):\n",
        "\n",
        "    groups = dataframe[group_column].unique()\n",
        "    for group in groups:\n",
        "\n",
        "        group_text = \" \".join(dataframe[dataframe[group_column] == group][text_column].dropna())\n",
        "\n",
        "        wordcloud = WordCloud(\n",
        "            max_font_size=50,\n",
        "            max_words=100,\n",
        "            background_color=\"white\"\n",
        "        ).generate(group_text)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        plt.title(f\"WordCloud for {group}\")\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "generate_wordcloud(df, text_column=\"statement\", group_column=\"status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWXFU09A4wSP"
      },
      "outputs": [],
      "source": [
        "label_map = {\n",
        "    'Normal': 0,\n",
        "    'Depression': 1,\n",
        "    'Suicidal': 2,\n",
        "    'Anxiety': 3,\n",
        "    'Bipolar': 4,\n",
        "    'Stress': 5,\n",
        "    'Personality disorder': 6\n",
        "}\n",
        "df[\"label\"] = df[\"status\"].map(label_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baA7RBTa48cx"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oa7wmGgo4HJ1"
      },
      "outputs": [],
      "source": [
        "df = df.drop([\"statement_length\", \"words\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uHu5i2j5Egp"
      },
      "outputs": [],
      "source": [
        "df= df.sample(frac=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV2GrXv15Q6l"
      },
      "outputs": [],
      "source": [
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwpbKVhgPEtE"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqIcOf0N5T-5"
      },
      "outputs": [],
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split( df[\"statement\"],\n",
        "                                                     df[\"label\"],\n",
        "                                                     random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efWOOuFM5WNE"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EYkV27SM5WK5"
      },
      "outputs": [],
      "source": [
        "train_encodings = bert_tokenizer(list(train_x), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")\n",
        "test_encodings = bert_tokenizer(list(test_x), truncation=True, padding=True, max_length=128, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPVjxVAi5WIW"
      },
      "outputs": [],
      "source": [
        "train_labels = torch.tensor(list(train_y))\n",
        "test_labels = torch.tensor(list(test_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wup8MGm-5WFn"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "test_dataset = CustomDataset(test_encodings, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pNBeYof5are"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=7)\n",
        "bert_model.train()\n",
        "\n",
        "optimizer = AdamW(bert_model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQGlq-Gn5apD"
      },
      "outputs": [],
      "source": [
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "bert_model.to(device)\n",
        "\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(15):\n",
        "    bert_model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss = loss_fn(logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        correct_predictions += (preds == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    print(f\"Epoch {epoch + 1} tamamlandı, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYuiemXu5amy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "bert_model.eval()\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predictions.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(classification_report(true_labels, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WccGhgVX5aj9"
      },
      "outputs": [],
      "source": [
        "text = \"My grades are always low. I am worried about my future.\"\n",
        "\n",
        "inputs = bert_tokenizer(\n",
        "    text,\n",
        "    max_length=128,\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    return_tensors=\"pt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HiOStNC5fDo"
      },
      "outputs": [],
      "source": [
        "bert_model.eval()\n",
        "\n",
        "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = bert_model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "print(\"Tahmin edilen sınıf:\", predicted_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBMTpp9P5g9P"
      },
      "outputs": [],
      "source": [
        "label_map_inverse = {v: k for k, v in label_map.items()}\n",
        "\n",
        "predicted_label = label_map_inverse.get(predicted_class, \"Unknown\")\n",
        "print(\"Tahmin edilen etiket:\", predicted_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHxKXqib9KVR"
      },
      "outputs": [],
      "source": [
        "llama_input = f\"Text: {text}\\nPredicted Label: {predicted_label}\\n\\nSummarize the text based on its content and predicted label.\"\n",
        "print(\"Llama Model Input:\", llama_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AmtnVlc5iW8"
      },
      "outputs": [],
      "source": [
        "bert_model.save_pretrained(\"./mental_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ga7EvRe5iY_"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer.save_pretrained(\"./mental_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbP6DSmY4Zbf"
      },
      "source": [
        "# LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSscDS1RZtnR"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(HF_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUa0WZR7ZvX4"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "XpMnp5Mak_Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S91GX7e5NklZ"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(llama_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=256,\n",
        "        num_beams=5,\n",
        "        early_stopping=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"Llama Model Output:\", generated_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOijJa6dkOsM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIc-xEX0dSMA"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def classify_and_summarize(input_text):\n",
        "    bert_inputs = bert_tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        bert_outputs = bert_model(**bert_inputs)\n",
        "        logits = bert_outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
        "        predicted_class = label_map_inverse.get(predicted_class, \"Unknown\")\n",
        "        predicted_label = f\"Class {predicted_class}\"\n",
        "\n",
        "    llama_input = f\"Text: {input_text}\\nPredicted Label: {predicted_label}\\n\\nSummarize the text based on its content and predicted label.\"\n",
        "    llama_inputs = tokenizer(llama_input, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        llama_outputs = model.generate(\n",
        "            input_ids=llama_inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=256,\n",
        "            num_beams=5,\n",
        "            early_stopping=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    summary = tokenizer.decode(llama_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return f\"Predicted Label: {predicted_label}\\n\\nSummary: {summary}\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Ruh Sağlığı Analizi \")\n",
        "    gr.Markdown(\"Aşağıya şikayetinizi girin ve sınıflandırmak ve özet için düğmeye tıklayınız.\")\n",
        "\n",
        "    input_text = gr.Textbox(label=\"Enter your text here\")\n",
        "\n",
        "    classify_button = gr.Button(\"Classify and Summarize\")\n",
        "\n",
        "    output_text = gr.Textbox(label=\"Result\")\n",
        "\n",
        "    classify_button.click(\n",
        "        fn=classify_and_summarize,\n",
        "        inputs=[input_text],\n",
        "        outputs=[output_text]\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}